---
title: "Kubernetesã‚’å®Œå…¨ã«ç†è§£ã—ãŸ(ç¬¬25å›) - 99.9999%ã‚’å®Ÿç¾ã™ã‚‹å®Œå…¨æ§‹æˆã€å®Œçµã€‘"
draft: true
tags:
- kubernetes
- production
- enterprise
- slo
- best-practices
description: 25å›ã‚·ãƒªãƒ¼ã‚ºã®é›†å¤§æˆã¨ã—ã¦ã€æœ€é«˜æ°´æº–ã®å¯ç”¨æ€§ã‚’æŒã¤Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã‚’å®Œæˆã•ã›ã¾ã™ã€‚å­¦ã‚“ã ã™ã¹ã¦ã®æŠ€è¡“ã‚’çµ±åˆã—ã€çœŸã®ç„¡æ•µã‚¤ãƒ³ãƒ•ãƒ©ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
---

## ã“ã‚Œã¾ã§ã®æ—…ã‚’æŒ¯ã‚Šè¿”ã£ã¦

ç¬¬1å›ã§ã€ŒKubernetesã£ã¦ä½•ï¼Ÿã€ã‹ã‚‰å§‹ã¾ã£ãŸã“ã®ã‚·ãƒªãƒ¼ã‚ºã‚‚ã€ã¤ã„ã«æœ€çµ‚å›ã‚’è¿ãˆã¾ã—ãŸã€‚

- **ç¬¬1-5å›**: åŸºç¤ç·¨ã§ã€Podã€Serviceã€Deploymentã€ConfigMapã€Secretã®åŸºæœ¬ã‚’å­¦ç¿’
- **ç¬¬6-10å›**: å®Ÿè·µç·¨ã§ã€Ingressã€StatefulSetã€DaemonSetã€Jobã€ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚’ç¿’å¾—
- **ç¬¬11-15å›**: ç›£è¦–ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç·¨ã§ã€Prometheusã€Grafanaã€RBACã€NetworkPolicyã€Admissionã‚’å®Ÿè£…
- **ç¬¬16-20å›**: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç·¨ã§ã€HPAã€VPAã€Cluster Autoscalerã€Node Affinityã€Auto Scalingã‚’å®Ÿç¾
- **ç¬¬21-24å›**: é«˜å¯ç”¨æ€§ç·¨ã§ã€Multi-Zoneã€Multi-Regionã€Chaos Engineeringã€GitOpsã‚’å®Œæˆ

ä»Šå›ã¯ã€ã“ã‚Œã¾ã§å­¦ã‚“ã ã™ã¹ã¦ã®æŠ€è¡“ã‚’çµ±åˆã—ã€**99.9999%(ã‚·ãƒƒã‚¯ã‚¹ãƒŠã‚¤ãƒ³)ã®å¯ç”¨æ€§**ã‚’æŒã¤ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®Kubernetesã‚¯ãƒ©ã‚¹ã‚¿ã‚’å®Œæˆã•ã›ã¾ã™ã€‚

## Six Ninesã®æ„å‘³

### å¯ç”¨æ€§ã®æ•°å€¤

å¯ç”¨æ€§99.9999%ã¨ã¯ã€å¹´é–“ã§ã‚ãšã‹**31.5ç§’**ã—ã‹ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ãŒè¨±ã•ã‚Œãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚

| å¯ç”¨æ€§ | å¹´é–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  | æœˆé–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  |
|--------|------------------|------------------|
| 99% (Two Nines) | 3.65æ—¥ | 7.2æ™‚é–“ |
| 99.9% (Three Nines) | 8.76æ™‚é–“ | 43.2åˆ† |
| 99.99% (Four Nines) | 52.56åˆ† | 4.32åˆ† |
| 99.999% (Five Nines) | 5.26åˆ† | 25.9ç§’ |
| **99.9999% (Six Nines)** | **31.5ç§’** | **2.59ç§’** |

### å®Ÿç¾ã«å¿…è¦ãªè¦ç´ 

Six Ninesã‚’é”æˆã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã™ã¹ã¦ãŒå¿…è¦ã§ã™ã€‚

1. **å†—é•·åŒ–**: å˜ä¸€éšœå®³ç‚¹ã®å®Œå…¨æ’é™¤
2. **è‡ªå‹•å¾©æ—§**: éšœå®³æ¤œçŸ¥ã‹ã‚‰å¾©æ—§ã¾ã§ã®è‡ªå‹•åŒ–
3. **é«˜é€Ÿãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼**: æ•°ç§’ä»¥å†…ã®åˆ‡ã‚Šæ›¿ãˆ
4. **ç¶™ç¶šçš„ãªãƒ†ã‚¹ãƒˆ**: ã‚«ã‚ªã‚¹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§ã®æ¤œè¨¼
5. **å®Œç’§ãªç›£è¦–**: éšœå®³ã®äºˆå…†æ¤œçŸ¥
6. **è‡ªå‹•åŒ–ã•ã‚ŒãŸé‹ç”¨**: äººçš„ãƒŸã‚¹ã®æ’é™¤

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å…¨ä½“åƒ

### ãƒãƒ«ãƒãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»ãƒãƒ«ãƒã‚¾ãƒ¼ãƒ³æ§‹æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Region: US-East â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€ Zone A â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€ Zone B â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Master Node 1  â”‚ â”‚ Master Node 2  â”‚       â”‚
â”‚  â”‚ Worker Nodes   â”‚ â”‚ Worker Nodes   â”‚       â”‚
â”‚  â”‚ etcd Member 1  â”‚ â”‚ etcd Member 2  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€ Zone C â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ Master Node 3  â”‚                           â”‚
â”‚  â”‚ Worker Nodes   â”‚                           â”‚
â”‚  â”‚ etcd Member 3  â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Region: EU-West â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (Similar 3-zone structure)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Region: AP-South â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (Similar 3-zone structure)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â†“ Global Load Balancer â†“
    (Cloudflare / AWS Global Accelerator)
```

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé…ç½®æˆ¦ç•¥

**ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³**:
- å„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã«3å°ã®ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰(å„ã‚¾ãƒ¼ãƒ³ã«1å°ãšã¤)
- etcdã‚¯ãƒ©ã‚¹ã‚¿ã¯3å°æ§‹æˆ(å¥‡æ•°å°ã§ã‚¯ã‚©ãƒ¼ãƒ©ãƒ ç¶­æŒ)
- HA Proxyã¾ãŸã¯Nginxã§APIã‚µãƒ¼ãƒãƒ¼ã‚’å†—é•·åŒ–

**ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰**:
- å„ã‚¾ãƒ¼ãƒ³ã«æœ€ä½3å°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰
- ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚°ãƒ«ãƒ¼ãƒ—ã§å‹•çš„æ‹¡å¼µ
- å¤šæ§˜ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã§ãƒªã‚¹ã‚¯åˆ†æ•£

## å®Œå…¨æ§‹æˆã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ

### ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: production
spec:
  replicas: 12  # å„ã‚¾ãƒ¼ãƒ³ã«4ã¤ãšã¤
  strategy:
    rollingUpdate:
      maxSurge: 3
      maxUnavailable: 0  # ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ 
  selector:
    matchLabels:
      app: web
      tier: frontend
  template:
    metadata:
      labels:
        app: web
        tier: frontend
    spec:
      # ãƒãƒ«ãƒã‚¾ãƒ¼ãƒ³åˆ†æ•£
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: web
      # ãƒãƒ¼ãƒ‰é–“åˆ†æ•£
      - maxSkew: 2
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: web
      # Podã‚¢ãƒ³ãƒã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: web
              topologyKey: kubernetes.io/hostname
      containers:
      - name: web
        image: myregistry.io/web-app:v1.0.0
        ports:
        - containerPort: 8080
          name: http
        # ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
        # Graceful Shutdown
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]
        # ç’°å¢ƒå¤‰æ•°
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: LOG_LEVEL
          value: "info"
        envFrom:
        - configMapRef:
            name: app-config
        - secretRef:
            name: app-secrets
---
# HPAè¨­å®š
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 12
  maxReplicas: 48
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
---
# PDBè¨­å®š
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb
  namespace: production
spec:
  minAvailable: 9  # å¸¸ã«75%ä»¥ä¸Šã‚’ç¶­æŒ
  selector:
    matchLabels:
      app: web
---
# Serviceè¨­å®š
apiVersion: v1
kind: Service
metadata:
  name: web-service
  namespace: production
  annotations:
    service.kubernetes.io/topology-aware-hints: auto
spec:
  type: ClusterIP
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
```

### ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: production
spec:
  serviceName: postgres
  replicas: 5  # ãƒ—ãƒ©ã‚¤ãƒãƒª1 + ãƒ¬ãƒ—ãƒªã‚«4
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      # ãƒãƒ«ãƒã‚¾ãƒ¼ãƒ³åˆ†æ•£
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: postgres
      # ãƒ›ã‚¹ãƒˆã‚¢ãƒ³ãƒã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: postgres
            topologyKey: kubernetes.io/hostname
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: data
          mountPath: /var/lib/postgresql/data
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pg_isready -U postgres
          initialDelaySeconds: 10
          periodSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd-replicated
      resources:
        requests:
          storage: 500Gi
---
# PDB for StatefulSet
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: postgres-pdb
  namespace: production
spec:
  minAvailable: 3  # æœ€ä½3å°ç¶­æŒ(ã‚¯ã‚©ãƒ¼ãƒ©ãƒ )
  selector:
    matchLabels:
      app: postgres
```

### Ingress Controller

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
  namespace: ingress-nginx
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
spec:
  type: LoadBalancer
  externalTrafficPolicy: Local  # Source IP preservation
  selector:
    app: nginx-ingress
  ports:
  - name: http
    port: 80
    targetPort: http
  - name: https
    port: 443
    targetPort: https
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  replicas: 9  # å„ã‚¾ãƒ¼ãƒ³ã«3ã¤ãšã¤
  selector:
    matchLabels:
      app: nginx-ingress
  template:
    metadata:
      labels:
        app: nginx-ingress
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: nginx-ingress
      containers:
      - name: nginx-ingress-controller
        image: k8s.gcr.io/ingress-nginx/controller:v1.9.0
        args:
        - /nginx-ingress-controller
        - --election-id=ingress-controller-leader
        - --controller-class=k8s.io/ingress-nginx
        - --configmap=$(POD_NAMESPACE)/nginx-configuration
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
          initialDelaySeconds: 10
          periodSeconds: 10
```

## ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆä½“ç³»

### SLI/SLO/SLAå®šç¾©

**ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«æŒ‡æ¨™(SLI)**:
- ãƒªã‚¯ã‚¨ã‚¹ãƒˆæˆåŠŸç‡: 99.99%ä»¥ä¸Š
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ (P95): 200msä»¥ä¸‹
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ (P99): 500msä»¥ä¸‹
- ã‚µãƒ¼ãƒ“ã‚¹å¯ç”¨æ€§: 99.9999%

**ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ç›®æ¨™(SLO)**:
```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: slo-rules
  namespace: monitoring
spec:
  groups:
  - name: slo
    interval: 30s
    rules:
    # ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆè¨ˆç®—
    - record: slo:error_budget_remaining
      expr: |
        1 - (
          sum(rate(http_requests_total{code=~"5.."}[30d]))
          /
          sum(rate(http_requests_total[30d]))
        )
    
    # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·SLO
    - record: slo:latency_p95
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
        )
    
    # å¯ç”¨æ€§SLO
    - record: slo:availability
      expr: |
        sum(up{job="web-app"}) / count(up{job="web-app"})
```

### å¤šå±¤ã‚¢ãƒ©ãƒ¼ãƒˆ

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: multi-tier-alerts
  namespace: monitoring
spec:
  groups:
  - name: critical-alerts
    rules:
    # Tier 1: å³åº§ã«å¯¾å¿œãŒå¿…è¦
    - alert: ServiceDown
      expr: up{job="web-app"} == 0
      for: 1m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Service {{ $labels.instance }} is down"
        
    - alert: ErrorBudgetExhausted
      expr: slo:error_budget_remaining < 0
      for: 5m
      labels:
        severity: critical
        tier: "1"
      annotations:
        summary: "Error budget exhausted - freeze deployments"
    
    # Tier 2: æ•°æ™‚é–“ä»¥å†…ã«å¯¾å¿œ
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{code=~"5.."}[5m])
        /
        rate(http_requests_total[5m])
        > 0.01
      for: 10m
      labels:
        severity: warning
        tier: "2"
      annotations:
        summary: "Error rate above 1%"
    
    # Tier 3: å–¶æ¥­æ™‚é–“å†…ã«å¯¾å¿œ
    - alert: HighLatency
      expr: slo:latency_p95 > 0.5
      for: 30m
      labels:
        severity: info
        tier: "3"
      annotations:
        summary: "P95 latency above 500ms"
```

## ç½å®³å¾©æ—§è¨ˆç”»

### ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆ¦ç•¥

```yaml
# Veleroå®šæœŸãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: production-backup
  namespace: velero
spec:
  schedule: "0 */6 * * *"  # 6æ™‚é–“ã”ã¨
  template:
    includedNamespaces:
    - production
    - ingress-nginx
    - monitoring
    storageLocation: default
    volumeSnapshotLocations:
    - default
    ttl: 720h0m0s  # 30æ—¥é–“ä¿æŒ
    hooks:
      resources:
      - name: postgres-backup-hook
        includedNamespaces:
        - production
        labelSelector:
          matchLabels:
            app: postgres
        pre:
        - exec:
            container: postgres
            command:
            - /bin/bash
            - -c
            - pg_dump -U postgres mydb > /tmp/backup.sql
            onError: Fail
```

### å¾©æ—§æ‰‹é †ã®è‡ªå‹•åŒ–

```bash
#!/bin/bash
# disaster-recovery.sh

BACKUP_NAME=$1
TARGET_REGION=$2

echo "Starting disaster recovery..."
echo "Backup: $BACKUP_NAME"
echo "Target Region: $TARGET_REGION"

# 1. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ã«åˆ‡ã‚Šæ›¿ãˆ
kubectl config use-context ${TARGET_REGION}-cluster

# 2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ãƒªã‚¹ãƒˆã‚¢
velero restore create --from-backup ${BACKUP_NAME} \
  --wait

# 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å¾©æ—§ç¢ºèª
kubectl wait --for=condition=ready pod -l app=postgres \
  --timeout=300s \
  -n production

# 4. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å¾©æ—§ç¢ºèª
kubectl wait --for=condition=ready pod -l app=web \
  --timeout=300s \
  -n production

# 5. DNSã‚’æ›´æ–°ã—ã¦ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’åˆ‡ã‚Šæ›¿ãˆ
aws route53 change-resource-record-sets \
  --hosted-zone-id ${HOSTED_ZONE_ID} \
  --change-batch file://failover-${TARGET_REGION}.json

echo "Disaster recovery completed!"
```

## GitOpsã«ã‚ˆã‚‹å®Œå…¨è‡ªå‹•åŒ–

### ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ã‚¿ç®¡ç†

```yaml
# ApplicationSet for all regions
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: global-deployment
  namespace: argocd
spec:
  generators:
  - list:
      elements:
      - cluster: us-east
        url: https://us-east.k8s.example.com
        replicas: "12"
      - cluster: eu-west
        url: https://eu-west.k8s.example.com
        replicas: "8"
      - cluster: ap-south
        url: https://ap-south.k8s.example.com
        replicas: "6"
  template:
    metadata:
      name: 'web-app-{{cluster}}'
    spec:
      project: production
      source:
        repoURL: https://github.com/myorg/k8s-manifests
        targetRevision: main
        path: apps/web-app/overlays/{{cluster}}
        helm:
          parameters:
          - name: replicaCount
            value: '{{replicas}}'
      destination:
        server: '{{url}}'
        namespace: production
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
        - CreateNamespace=true
        retry:
          limit: 5
          backoff:
            duration: 5s
            factor: 2
            maxDuration: 3m
```

## ã‚³ã‚¹ãƒˆæœ€é©åŒ–

### ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡åŒ–

```yaml
# VPA for right-sizing
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: web-vpa
  namespace: production
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: web
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2000m
        memory: 2Gi
      controlledResources:
      - cpu
      - memory
```

### Spot/Preemptibleã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®æ´»ç”¨

```yaml
# Mixed instance types node pool
apiVersion: v1
kind: Node
metadata:
  labels:
    node.kubernetes.io/instance-type: mixed
    capacity-type: spot
spec:
  taints:
  - key: spot
    value: "true"
    effect: NoSchedule
---
# Toleration for spot instances
apiVersion: apps/v1
kind: Deployment
metadata:
  name: batch-processor
spec:
  template:
    spec:
      tolerations:
      - key: spot
        operator: Equal
        value: "true"
        effect: NoSchedule
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: capacity-type
                operator: In
                values:
                - spot
```

## ã‚«ã‚ªã‚¹ãƒ†ã‚¹ãƒˆã®ç¶™ç¶šå®Ÿæ–½

### æ¯é€±ã®GameDay

```yaml
apiVersion: litmuschaos.io/v1alpha1
kind: ChaosSchedule
metadata:
  name: production-gameday
  namespace: production
spec:
  schedule:
    repeat:
      timeRange:
        startTime: "2024-01-01T14:00:00Z"
        endTime: "2024-12-31T16:00:00Z"
      properties:
        minChaosInterval: "168h"  # æ¯é€±
      workDays:
        includedDays: "Tue"  # ç«æ›œæ—¥ã«å®Ÿæ–½
  engineTemplateSpec:
    appinfo:
      appns: production
      applabel: "tier=frontend"
      appkind: deployment
    engineState: active
    chaosServiceAccount: chaos-sa
    experiments:
    - name: pod-delete
    - name: pod-network-latency
    - name: pod-cpu-hog
```

## é”æˆã—ãŸæœ€çµ‚æ§‹æˆ

### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [x] **å†—é•·æ€§**: 3ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ Ã— 3ã‚¾ãƒ¼ãƒ³æ§‹æˆ
- [x] **ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ãƒ¼ãƒ³**: å„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³3å°ã®ãƒã‚¹ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰
- [x] **ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰**: ã‚¾ãƒ¼ãƒ³ã”ã¨ã«æœ€ä½3å°
- [x] **ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ã‚¢ãƒ—ãƒª**: 12ãƒ¬ãƒ—ãƒªã‚«ä»¥ä¸Šã€HPA/VPAå¯¾å¿œ
- [x] **ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ã‚¢ãƒ—ãƒª**: 5ãƒ¬ãƒ—ãƒªã‚«ã€ãƒãƒ«ãƒã‚¾ãƒ¼ãƒ³åˆ†æ•£
- [x] **Ingress**: 9ãƒ¬ãƒ—ãƒªã‚«ã€ã‚¯ãƒ­ã‚¹ã‚¾ãƒ¼ãƒ³LB
- [x] **ç›£è¦–**: Prometheus/Grafana/Alertmanager
- [x] **ãƒ­ã‚°**: Lokié›†ç´„ã€é•·æœŸä¿å­˜
- [x] **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: RBACã€NetworkPolicyã€PodSecurityPolicy
- [x] **GitOps**: ArgoCDã€å®Œå…¨è‡ªå‹•åŒæœŸ
- [x] **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: 6æ™‚é–“ã”ã¨ã®Veleroãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
- [x] **ã‚«ã‚ªã‚¹ãƒ†ã‚¹ãƒˆ**: æ¯é€±ã®å®šæœŸå®Ÿé¨“
- [x] **SLOç›£è¦–**: ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆè¿½è·¡
- [x] **ç½å®³å¾©æ—§**: è‡ªå‹•åŒ–ã•ã‚ŒãŸå¾©æ—§æ‰‹é †

### äºˆæƒ³ã•ã‚Œã‚‹å¯ç”¨æ€§

ç†è«–å€¤ã®è¨ˆç®—:

```
Single Pod: 99.9%
12 Replicas across 3 zones: 1 - (0.001^12) â‰ˆ 99.999999999%

With network (99.99%): 99.999999999% Ã— 0.9999 â‰ˆ 99.9999%
With human operations (99.9%): 99.9999% Ã— 0.999 â‰ˆ 99.8999%

Realistic SLA: 99.99% (Four Nines)
Stretch Goal: 99.999% (Five Nines)
Theoretical Max: 99.9999% (Six Nines)
```

å®Ÿéš›ã«ã¯ã€äººçš„ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯éšœå®³ã‚’è€ƒæ…®ã™ã‚‹ã¨**99.99%(Four Nines)ãŒç¾å®Ÿçš„ãªç›®æ¨™**ã§ã‚ã‚Šã€ã“ã‚Œã§ã‚‚å¹´é–“52.56åˆ†ã®ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã¨ã„ã†ç´ æ™´ã‚‰ã—ã„æ°´æº–ã§ã™ã€‚

## ã‚·ãƒªãƒ¼ã‚ºç·æ‹¬

### 25å›ã§å­¦ã‚“ã ã“ã¨

ã“ã®ã‚·ãƒªãƒ¼ã‚ºã‚’é€šã˜ã¦ã€ä»¥ä¸‹ã®ã‚¹ã‚­ãƒ«ã‚’ç¿’å¾—ã—ã¾ã—ãŸã€‚

**åŸºç¤çŸ¥è­˜**:
- Kubernetesã®åŸºæœ¬ã‚³ãƒ³ã‚»ãƒ—ãƒˆ(Podã€Serviceã€Deployment)
- ã‚³ãƒ³ãƒ†ãƒŠã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸç†
- å®£è¨€çš„ãªè¨­å®šç®¡ç†

**å®Ÿè·µã‚¹ã‚­ãƒ«**:
- ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã®ä½œæˆã¨ç®¡ç†
- Helmãƒãƒ£ãƒ¼ãƒˆã®æ´»ç”¨
- Kustomizeã«ã‚ˆã‚‹ç’°å¢ƒåˆ¥è¨­å®š

**é‹ç”¨ãƒã‚¦ãƒã‚¦**:
- ç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°ã®å®Ÿè£…
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- GitOpsã«ã‚ˆã‚‹è‡ªå‹•åŒ–

**é«˜åº¦ãªæŠ€è¡“**:
- ãƒãƒ«ãƒã‚¾ãƒ¼ãƒ³/ãƒãƒ«ãƒãƒªãƒ¼ã‚¸ãƒ§ãƒ³æ§‹æˆ
- Auto Scalingæˆ¦ç•¥
- ã‚«ã‚ªã‚¹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

### ã“ã‚Œã‹ã‚‰ã®å­¦ç¿’

Kubernetesã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¯æ—¥ã€…é€²åŒ–ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«å­¦ã¶ã¹ããƒˆãƒ”ãƒƒã‚¯:

- **Service Mesh**: Istioã€Linkerdã«ã‚ˆã‚‹é«˜åº¦ãªãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ¶å¾¡
- **Serverless**: Knativeã«ã‚ˆã‚‹FaaS
- **AI/ML**: Kubeflowã«ã‚ˆã‚‹MLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- **Edge Computing**: K3sã€MicroK8sã«ã‚ˆã‚‹ã‚¨ãƒƒã‚¸å±•é–‹
- **Platform Engineering**: Crossplaneã€Backstageã«ã‚ˆã‚‹å†…éƒ¨ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ§‹ç¯‰

### æœ€å¾Œã«

Kubernetesã¯è¤‡é›‘ã§ã™ãŒã€ä¸€æ­©ãšã¤å­¦ã¹ã°å¿…ãšç†è§£ã§ãã¾ã™ã€‚ã“ã®ã‚·ãƒªãƒ¼ã‚ºã§åŸºç¤ã‹ã‚‰é«˜åº¦ãªæŠ€è¡“ã¾ã§ä½“ç³»çš„ã«å­¦ã¶ã“ã¨ãŒã§ããŸã¯ãšã§ã™ã€‚

**é‡è¦ãªã®ã¯ã€ã™ã¹ã¦ã‚’ä¸€åº¦ã«å®Ÿè£…ã—ã‚ˆã†ã¨ã—ãªã„ã“ã¨**ã§ã™ã€‚ã¾ãšã¯åŸºæœ¬ã‹ã‚‰å§‹ã‚ã€æ®µéšçš„ã«é«˜åº¦ãªæ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã„ãã“ã¨ãŒæˆåŠŸã®éµã§ã™ã€‚

ãã—ã¦ã€**å¤±æ•—ã‚’æã‚Œãªã„ã“ã¨**ã€‚ã‚«ã‚ªã‚¹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§å­¦ã‚“ã ã‚ˆã†ã«ã€éšœå®³ã¯å­¦ã³ã®æ©Ÿä¼šã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§åˆã‚ã¦é­é‡ã™ã‚‹ã‚ˆã‚Šã‚‚ã€é–‹ç™ºç’°å¢ƒã‚„ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã§ç©æ¥µçš„ã«è©¦ã—ã¦å¤±æ•—ã—ãŸæ–¹ãŒé¥ã‹ã«ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚

## ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸï¼

25å›ã«ã‚ãŸã‚‹é•·ã„ã‚·ãƒªãƒ¼ã‚ºã‚’æœ€å¾Œã¾ã§èª­ã‚“ã§ã„ãŸã ãã€æœ¬å½“ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸã€‚

ã“ã®ã‚·ãƒªãƒ¼ã‚ºãŒã€çš†ã•ã‚“ã®Kuberneteså­¦ç¿’ã®ä¸€åŠ©ã¨ãªã‚Šã€æœ¬ç•ªç’°å¢ƒã§çœŸã«ä¿¡é ¼æ€§ã®é«˜ã„ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹åŠ›ã«ãªã‚Œã°å¹¸ã„ã§ã™ã€‚

**Kubernetesã‚’å®Œå…¨ã«ç†è§£ã—ãŸ**ã‚ãªãŸã¯ã€ã‚‚ã†ç„¡æ•µã§ã™ã€‚è‡ªä¿¡ã‚’æŒã£ã¦ã€ä¸–ç•Œã‚’å¤‰ãˆã‚‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ï¼

---

### å‚è€ƒãƒªã‚½ãƒ¼ã‚¹

- Kuberneteså…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: https://kubernetes.io/docs/
- CNCF Landscape: https://landscape.cncf.io/
- Kubernetes Patterns (O'Reillyæ›¸ç±)
- Production Kubernetes (O'Reillyæ›¸ç±)
- Google SRE Book: https://sre.google/books/

### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£

- Kubernetes Slack: https://slack.k8s.io/
- Stack Overflow [kubernetes]ã‚¿ã‚°
- CNCF Events & Meetups

Happy Kubernetes Journey! ğŸš€
