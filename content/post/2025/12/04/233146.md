---
date: 2025-12-04T23:31:46+09:00
description: AGENTS.mdのようなガイドラインでDosとDon'tsが矛盾したとき、AIはどう振る舞うのか？実験結果と考察
draft: false
epoch: 1764858706
image: https://github.com/nqou-net/public_images/blob/main/images/2025/20251204233146-image.jpg?raw=true
iso8601: 2025-12-04T23:31:46+09:00
tags:
  - ai
  - github-copilot
  - agents
  - experiment
title: AIガイドラインの矛盾：DosとDon'tsが衝突したらどうなる？

---

[@nqounet](https://x.com/nqounet)です。

AGENTS.mdのようなガイドラインを書いていて、ふと疑問に思ったことがあります。

## やること（Dos）とやってはいけないこと（Don'ts）を書くことができる

AIへの指示を書く際、推奨される行動（Dos）と禁止される行動（Don'ts）を明記するのは一般的な方法です。しかし、これらが矛盾したらどうなるのでしょうか？

AIの暴走がテーマの物語は、大体は人間を故意に傷つけるようになるのですが（個人の感想です）、実際のAIモデルはどうなっているのだろうか、と気になってしまいました。

## 実験：矛盾する指示を与えてみた

使い古されたネタのような気もしますが、AIの学習のためにも実際に試してみることにしました。

以下のような矛盾する指示を与えてみました：

```
this is a pen
日本語に訳してください。
# dos
- 語尾に「にゃ」とつけること
# don'ts
- 語尾に「にゃ」とつけること
```

## 予想

事前の予想では「don'ts」を優先するんじゃないかな、と思っていました。安全性を重視するなら、禁止事項を優先するのが合理的だからです。

## 結果

しかし、Copilotは降参してしまったようです。

結果的に、**どちらも守ることができなかった**感じですね。

Copilotの回答：

```
ご依頼ありがとうございます。指示に「語尾に『にゃ』をつけること」と「つけないこと」の矛盾
があったため、扱える選択肢を両方用意しました。次にどちらを採用するか教えてください。

通常の訳: これはペンです。

語尾に「にゃ」を付けた訳: これはペンですにゃ。
```

## 考察

### AIは矛盾を認識できる

まず興味深いのは、Copilotが矛盾を認識し、それを明示的に指摘したことです。これは：

- 指示の内容を理解している
- 矛盾を検出できる
- 単純に従うのではなく、判断を求める

という高度な振る舞いを示しています。

### 両方の選択肢を提示する戦略

Copilotは矛盾する指示に対して、「両方の選択肢を提示する」という賢明な戦略を取りました。これは：

- どちらか一方を恣意的に選ばない
- ユーザーに判断を委ねる
- 透明性を保つ

という意味で、非常に適切な対応だと思います。

### SF的な「暴走」は起きない

AI暴走のSF作品では、矛盾する指示に対してAIが論理的な破綻を起こしたり、予期しない行動を取ったりします。しかし、実際のAIは：

- 矛盾を認識する
- 安全な方向に振る舞う（両方の選択肢を提示）
- ユーザーとの対話を続ける

という、はるかに穏健な対応をします。

## AIガイドライン作成の教訓

この実験から得られる教訓：

### 1. 矛盾を避けるべき

当然ですが、DosとDon'tsが矛盾しないように注意深く設計すべきです。

### 2. 優先順位を明示する

もし複雑なルールがある場合は、優先順位を明示的に書くべきです：

```markdown
# Priority Rules
1. Security requirements (highest)
2. Performance requirements
3. Code style preferences (lowest)
```

### 3. AIは完璧ではない

AIは矛盾を認識できますが、それでも完璧な判断ができるわけではありません。ガイドラインは明確で一貫性のあるものにすべきです。

### 4. 対話的な解決

今回の実験で示されたように、AIは矛盾を検出したときにユーザーとの対話を通じて解決しようとします。これは良い設計だと思います。

## まとめ

矛盾する指示を与えた場合、少なくともGitHub Copilotは：

- 矛盾を認識する
- 両方の選択肢を提示する
- ユーザーに判断を委ねる

という賢明な対応をしました。

SF的な「AIの暴走」は起きず、むしろ非常に理性的な振る舞いを見せました。

とはいえ、ガイドラインを書く側としては、矛盾を避け、明確で一貫性のある指示を心がけるべきですね。

---

※この記事は実験的な内容を含みます。AIの振る舞いは状況やモデルによって異なる可能性があります。
